{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Filter out specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\".*TDS_COLMETADATA.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ========== Setting the Java Home Env\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print('|',\"=\"*10,\"Setting the Java Home Env\")\n",
    "os.environ['JAVA_HOME']='/Library/Java/JavaVirtualMachines/jdk-21.jdk/Contents/Home'\n",
    "os.environ['SPARK_HOME']='/Users/mac/spark'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS']= \"notebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ========== Importing libraries\n"
     ]
    }
   ],
   "source": [
    "print('|',\"=\"*10,\"Importing libraries\")\n",
    "# import findspark\n",
    "# import pyspark\n",
    "# findspark.init()\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import os,duckdb,sys\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "# Suppress specific warning\n",
    "# warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pyspark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ========== Create a spark session\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/17 14:43:33 WARN Utils: Your hostname, Senyonjos-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.100.37 instead (on interface en0)\n",
      "23/12/17 14:43:33 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/12/17 14:43:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/17 14:43:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "print('|',\"=\"*10,\"Create a spark session\")\n",
    "spark = SparkSession.builder \\\n",
    "                    .appName('Duck to MSSQL') \\\n",
    "                    .config('spark.jars', '/Users/mac/Downloads/sqljdbc_12.4/enu/jars/mssql-jdbc-12.4.2.jre11.jar') \\\n",
    "                    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.100.37:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Duck to MSSQL</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x110cab710>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| ========== Loading data  from Dudckdb to MSSQL\n",
      "1. Reading stg data from duckdb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/12/17 14:43:47 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "23/12/17 14:43:48 WARN TOKEN: ConnectionID:3 ClientConnectionId: bd43be67-a111-4dcb-9b56-4da390acdaf9: SQLServerConnection.supportsTransactions: Discarding unexpected TDS_COLMETADATA (0x81)\n",
      "23/12/17 14:43:48 WARN TOKEN: ConnectionID:5 ClientConnectionId: 4ff8d047-0e44-4b61-a901-4efacb6c3932: SQLServerConnection.supportsTransactions: Discarding unexpected TDS_COLMETADATA (0x81)\n",
      "23/12/17 14:43:48 WARN TOKEN: ConnectionID:6 ClientConnectionId: 72b2fd0d-2f30-4d23-9799-588f4c61e3a5: SQLServerConnection.supportsTransactions: Discarding unexpected TDS_COLMETADATA (0x81)\n",
      "23/12/17 14:43:48 WARN TOKEN: ConnectionID:10 ClientConnectionId: 2fb023c2-667a-4205-853d-bd05d7da4d8a: SQLServerConnection.supportsTransactions: Discarding unexpected TDS_COLMETADATA (0x81)\n",
      "23/12/17 14:43:48 WARN TOKEN: ConnectionID:7 ClientConnectionId: a56467f8-1b65-4ad8-b9ac-f7c36432ca25: SQLServerConnection.supportsTransactions: Discarding unexpected TDS_COLMETADATA (0x81)\n",
      "23/12/17 14:43:48 WARN TOKEN: ConnectionID:8 ClientConnectionId: d3e683c1-64d2-4218-ae60-a2b98fb27db6: SQLServerConnection.supportsTransactions: Discarding unexpected TDS_COLMETADATA (0x81)\n",
      "23/12/17 14:43:48 WARN TOKEN: ConnectionID:9 ClientConnectionId: 2b551e90-e67c-4da7-bf84-672ce9154d0c: SQLServerConnection.supportsTransactions: Discarding unexpected TDS_COLMETADATA (0x81)\n",
      "23/12/17 14:43:48 WARN TOKEN: ConnectionID:4 ClientConnectionId: e0ec4527-565b-47d0-8f08-c23b38099009: SQLServerConnection.supportsTransactions: Discarding unexpected TDS_COLMETADATA (0x81)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!Transactions Table loaded successfully\n"
     ]
    }
   ],
   "source": [
    "print('|',\"=\"*10,\"Loading data  from Dudckdb to MSSQL\")\n",
    "print('1.',\"Reading stg data from duckdb\")\n",
    "duckdb_path='./engineering.duckdb'\n",
    "#conneting to duckdb\n",
    "connection=duckdb.connect(database=str(duckdb_path))\n",
    "df=connection.sql(\"SELECT * FROM STG_TRANSACTIONS\").to_df()\n",
    "#convert results to pandas Dataframe\n",
    "# df=df.fetch_df()S\n",
    "#convert the result to a pyspark dataframe\n",
    "df=spark.createDataFrame(df)\n",
    "connection.close()\n",
    "\n",
    "#now that we have an RDD , write it to msssql data\n",
    "properties = {\n",
    "    \"user\": \"sa\",\n",
    "    \"password\": \"K@roukazeno1\",\n",
    "    \"driver\": \"com.microsoft.sqlserver.jdbc.SQLServerDriver\",\n",
    "    \"encrypt\": \"true\",\n",
    "    \"trustServerCertificate\": \"true\"\n",
    "}\n",
    "\n",
    "# Correct the JDBC URL\n",
    "jdbc_url = \"jdbc:sqlserver://localhost:1433;databaseName=ENGINEERING\"\n",
    "\n",
    "# df.show()\n",
    "\n",
    "#readinng data from warehouse \n",
    "# acc = spark.read \\\n",
    "#     .jdbc(url=jdbc_url, table=\"(SELECT TOP 10 * FROM ACCOUNTS)\", properties=properties)\n",
    "\n",
    "try:\n",
    "    acc = spark.read \\\n",
    "        .jdbc(url=jdbc_url, table=\"ACCOUNTS\", properties=properties)\n",
    "    # acc.show()\n",
    "except Exception as e:\n",
    "    print(\"An error occurred:\", e)\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "# acc.show()\n",
    "\n",
    "# Loading the data to the warehouse\n",
    "try:\n",
    "    df.write \\\n",
    "    .mode('overwrite') \\\n",
    "    .jdbc(url=jdbc_url, table=\"TRANSACTIONS\", properties=properties)\n",
    "    print('!Transactions Table loaded successfully')\n",
    "except Exception as e:\n",
    "    print(\"Error:\", e)\n",
    "    raise e\n",
    "finally:\n",
    "    spark.stop()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
